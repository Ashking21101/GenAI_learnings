{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157bdf20",
   "metadata": {},
   "source": [
    "# Building a Chatbot\n",
    "\n",
    "conversation chatbot, remembers previous interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ca583e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ca8a4704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021D2774D1E0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021D2774CFD0>, root_client=<openai.OpenAI object at 0x0000021D2774F1C0>, root_async_client=<openai.AsyncOpenAI object at 0x0000021D2774F190>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model = 'gpt-4o')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fed1e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Ashish! It's great to meet you. How's your journey with GenAI going so far? If you have any questions or need assistance, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 20, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_df0f7b956c', 'id': 'chatcmpl-C4oWLm414uDpSNAZkFRFfSMqXsdIb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3db5932c-82ed-44be-87da-d5ee74768292-0', usage_metadata={'input_tokens': 20, 'output_tokens': 35, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "625f961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ashish, and you mentioned that you are a GenAI learner. If there‚Äôs anything specific you‚Äôd like to delve into or explore further, just let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 82, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-C4oWNrLr7WiHy1n1hAsVyZ9ohf8dv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--edef59ec-5df4-45c0-93ef-afc1cdb23175-0', usage_metadata={'input_tokens': 82, 'output_tokens': 37, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\"),\n",
    "    AIMessage(content=\"Hello Ashish! It's great to meet you. If you're a GenAI learner, feel free to ask any questions you might have about artificial intelligence, machine learning, or any related topics. I'm here to help!\"),\n",
    "    HumanMessage(content=\"hey whats my name, and what am I learning?\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3859b",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43312af2",
   "metadata": {},
   "source": [
    "1. ChatMessageHistory\n",
    "\n",
    "This is the actual storage for the messages.\n",
    "\n",
    "It records every message in the conversation (both from the user and the AI).\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "2. BaseChatMessageHistory (mean what methods chould ChatMessageHistory Should have , its kinda blueprint for it)\n",
    "\n",
    "This is not the storage itself ‚Äî it‚Äôs more like the rules or interface for what ‚Äúa message history‚Äù should be able to do.\n",
    "\n",
    "It defines how any kind of message history should behave (methods like add a message, get all messages).\n",
    "\n",
    "ChatMessageHistory inherits from this, meaning it follows those rules but also actually implements them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4c1b1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_message_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_message_history)\n",
    "\n",
    "\n",
    "# ChatMessageHistory stores all the messages in a conversation\n",
    "# BaseChatMessageHistory is a blueprint for ChatMessageHistory, it says what methods should be implemented on ChatMessageHistory\n",
    "\n",
    "# def get_message_history(session_id:str) -> BaseChatMessageHistory: \n",
    "# Means that whatever the fucntion returns, it should get BaseChatMessageHistory methods (without it, it returns a ChatMessageHistory object)\n",
    "# if session_id not in store: Then initialize the store with a new ChatMessageHistory object\n",
    "# Else return store[session_id] Returns the ChatMessageHistory object for the given session_id\n",
    "\n",
    "# RunnableWithMessageHistory  Wrap the model so it can store and retrieve past conversation messages.\n",
    "# Without this, the model will only see the current input and forget earlier ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "345f9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cb7bdc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ashish! It's great to meet you. How can I assist you in your journey of learning about Generative AI?\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")],\n",
    "\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2cd1e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Ashish! Welcome to the world of Generative AI. If you have any questions or need help with anything related to GenAI, feel free to ask. How can I assist you today?'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "response.content\n",
    "\n",
    "# as long as we use same session_id, it will remember \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae5041",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be61ab",
   "metadata": {},
   "source": [
    ".from_messages() and .from_template() are optional helpers, not requirements.\n",
    "\n",
    "You can create a ChatPromptTemplate by directly passing message templates to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2f082f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer user's question to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name = \"history\") \n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "#Yes ‚Äî in LangChain, a MessagesPlaceholder is the spot in your prompt where the chat history will be inserted.\n",
    "\n",
    "# history / messages / conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16078a6c",
   "metadata": {},
   "source": [
    "What you‚Äôre seeing is the difference between:\n",
    "\n",
    "Plain chain with MessagesPlaceholder ‚Üí you must pass \"history\" manually.\n",
    "\n",
    "Chain wrapped with RunnableWithMessageHistory ‚Üí you do not pass \"history\" anymore. It is injected for you automatically.\n",
    "\n",
    "Why it works without you passing history\n",
    "\n",
    "RunnableWithMessageHistory is a wrapper.\n",
    "\n",
    "It intercepts calls to .invoke() and does 2 extra things:\n",
    "\n",
    "Looks at the config[\"configurable\"][\"session_id\"] you provided.\n",
    "\n",
    "Calls your get_message_history(session_id) function.\n",
    "\n",
    "Fetches the stored messages (past history) and automatically supplies them into the variable that matches your MessagesPlaceholder (in your case \"history\").\n",
    "\n",
    "After the model responds, it updates that session‚Äôs history (so the next call has more context).\n",
    "\n",
    "So the \"history\" placeholder is still being filled ‚Äî just not by you manually. Instead, the wrapper does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "91ce5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b24c9071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ashish! It‚Äôs great to meet you. It's exciting that you‚Äôre learning about Generative AI. If you have any questions or need any help with your studies, feel free to ask. What specific aspects of Generative AI are you interested in?\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf77aa6",
   "metadata": {},
   "source": [
    "### more complexity(for demo, not using with_message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926d4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§∂‡•Ä‡§∑! ‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§è‡§ï ‡§ú‡§®‡§∞‡•á‡§ü‡§ø‡§µ ‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡§ø‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏ (GenAI) ‡§ï‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§á‡§∏ ‡§µ‡§ø‡§∑‡§Ø ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§Ø‡§æ ‡§Ü‡§™‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ï‡•ã‡§à ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§π‡•à? ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¨‡§§‡§æ‡§è‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Ç‡§ó‡§æ‡•§'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "    (\"system\", \"You are a helpful assistant. Answer user's question to the best of your abilityin {language}\"),\n",
    "    MessagesPlaceholder(variable_name= \"history\")# imporant to pass in Runnable.. , along with new question\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"history\":[HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")],  \n",
    "        \"language\":\"Hindi\"\n",
    "    })\n",
    "# Key-value pairs  \n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb257a1",
   "metadata": {},
   "source": [
    "# lets wrap, coz we will add multiple keys in with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0356e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "    (\"system\", \"You are a helpful assistant. Answer user's question to the best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name= \"history\"), # imporant to pass in Runnable.. , along with new question\n",
    "    (\"human\",\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e218e",
   "metadata": {},
   "source": [
    "When you wrap a chain with RunnableWithMessageHistory, it has to know two things:\n",
    "\n",
    "Which input is the new human message (so it can log it into history after the run).\n",
    "üëâ That‚Äôs why you set input_messages_key=\"question\" (since in your prompt the human turn is (\"{human}\", \"{question}\")).\n",
    "\n",
    "Which variable is reserved for past messages (the MessagesPlaceholder).\n",
    "üëâ That‚Äôs why you set history_messages_key=\"history\" (because your placeholder is MessagesPlaceholder(variable_name=\"history\"))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "40f483ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key = \"history\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79802e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§∂‡•Ä‡§∑! ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à‡•§ ‡§ú‡§®‡§∞‡•á‡§ü‡§ø‡§µ ‡§è‡§Ü‡§à ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∞‡•Å‡§ö‡§ø ‡§¶‡•á‡§ñ ‡§ï‡§∞ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§æ‡•§ ‡§Ö‡§ó‡§∞ ‡§Ü‡§™‡§ï‡•á ‡§™‡§æ‡§∏ ‡§á‡§∏ ‡§µ‡§ø‡§∑‡§Ø ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡•ã‡§à ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ø‡§æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è, ‡§§‡•ã ‡§¨‡•á‡§ù‡§ø‡§ù‡§ï ‡§™‡•Ç‡§õ‡•á‡§Ç‡•§ ‡§Æ‡•à‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å!'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"question\":[HumanMessage(content=\"Hi, my name is ashish. I am GenAi learner\")], \n",
    "        \"language\":\"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e039dc",
   "metadata": {},
   "source": [
    "New question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "11622be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§è‡§ï ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ï‡§æ ‡§ó‡§π‡§® ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§Æ‡•â‡§°‡§≤ (‡§°‡•Ä‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤) ‡§π‡•à ‡§ú‡•ã ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§™‡•ç‡§∞‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ (‡§è‡§®‡§è‡§≤‡§™‡•Ä) ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§ú‡§º‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡•á 2017 ‡§Æ‡•á‡§Ç \"Attention is All You Need\" ‡§®‡§æ‡§Æ‡§ï ‡§è‡§ï ‡§∂‡•ã‡§ß ‡§™‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§ó‡•Ç‡§ó‡§≤ ‡§ï‡•á ‡§∂‡•ã‡§ß‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§®‡•á ‡§è‡§®‡§è‡§≤‡§™‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§∞‡§æ‡§Ç‡§§‡§ø ‡§≤‡§æ ‡§¶‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§Ü‡§ú‡§ï‡§≤ ‡§ï‡•á ‡§∏‡§¨‡§∏‡•á ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§ú‡•à‡§∏‡•á ‡§ï‡§ø GPT, BERT, ‡§Ü‡§¶‡§ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§\\n\\n‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§ï ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§π‡•à‡§Ç:\\n\\n1. **‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§∞-‡§°‡§ø‡§ï‡•ã‡§°‡§∞ ‡§Ü‡§∞‡•ç‡§ï‡§ø‡§ü‡•á‡§ï‡•ç‡§ö‡§∞**: ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§∞ ‡§î‡§∞ ‡§°‡§ø‡§ï‡•ã‡§°‡§∞ ‡§ñ‡§Ç‡§° ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§∞ ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡•á ‡§ï‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§¨‡§ï‡§ø ‡§°‡§ø‡§ï‡•ã‡§°‡§∞ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§\\n\\n2. **‡§∏‡•á‡§≤‡•ç‡§´-‡§Ö‡§ü‡•á‡§Ç‡§∂‡§® ‡§Æ‡•à‡§ï‡•á‡§®‡§ø‡§ú‡•ç‡§Æ**: ‡§Ø‡§π ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ò‡§ü‡§ï ‡§π‡•à‡•§ ‡§∏‡•á‡§≤‡•ç‡§´-‡§Ö‡§ü‡•á‡§Ç‡§∂‡§® ‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§∏‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∂‡§¨‡•ç‡§¶ ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§â‡§∏‡•á ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ ‡§ï‡•á ‡§Ö‡§®‡•ç‡§Ø ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§∏‡•á ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§î‡§∞ ‡§∏‡§Æ‡§ù ‡§¨‡§®‡§§‡•Ä ‡§π‡•à‡•§\\n\\n3. **‡§™‡•â‡§ú‡§º‡§ø‡§∂‡§®‡§≤ ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó**: ‡§ö‡•Ç‡§Ç‡§ï‡§ø ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§ø‡§ï ‡§°‡•á‡§ü‡§æ ‡§ï‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§™‡§¢‡§º‡§§‡•á, ‡§á‡§∏‡§≤‡§ø‡§è ‡§™‡•â‡§ú‡§º‡§ø‡§∂‡§®‡§≤ ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§≤ ‡§∏‡§ï‡•á‡•§\\n\\n4. **‡§´‡•Ä‡§°-‡§´‡•â‡§∞‡§µ‡§∞‡•ç‡§° ‡§≤‡•á‡§Ø‡§∞‡•ç‡§∏**: ‡§π‡§∞ ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§∞ ‡§î‡§∞ ‡§°‡§ø‡§ï‡•ã‡§°‡§∞ ‡§ï‡•á ‡§≠‡•Ä‡§§‡§∞ ‡§è‡§ï ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§´‡•Ä‡§°-‡§´‡•â‡§∞‡§µ‡§∞‡•ç‡§° ‡§®‡•ç‡§Ø‡•Ç‡§∞‡§≤ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§ï‡§ø ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡§¨‡•ç‡§¶ ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø‡§§‡•ç‡§µ (‡§µ‡•á‡§ï‡•ç‡§ü‡§∞) ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§\\n\\n‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞ ‡§®‡•á ‡§ï‡§à ‡§è‡§®‡§è‡§≤‡§™‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ú‡•à‡§∏‡•á ‡§≠‡§æ‡§∑‡§æ‡§Ç‡§§‡§∞‡§£, ‡§∏‡§µ‡§æ‡§≤-‡§ú‡§µ‡§æ‡§¨, ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ú‡§®‡§∞‡•á‡§∂‡§® ‡§Ü‡§¶‡§ø ‡§Æ‡•á‡§Ç ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ‡§®‡•Ä‡§Ø ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç‡•§ ‡§á‡§®‡§ï‡•á ‡§∏‡•á‡§≤‡•ç‡§´-‡§Ö‡§ü‡•á‡§Ç‡§∂‡§® ‡§î‡§∞ ‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§ø‡§≤‡§ø‡§ü‡•Ä ‡§®‡•á ‡§á‡§®‡•ç‡§π‡•á‡§Ç ‡§ó‡§π‡§® ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•á ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§¨‡•á‡§π‡§¶ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§¨‡§®‡§æ ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ó‡§∞ ‡§Ü‡§™‡§ï‡•á ‡§™‡§æ‡§∏ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¨‡§§‡§æ‡§è‡§Ç!'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"question\":[HumanMessage(content=\"explain transformer\")], \n",
    "        \"language\":\"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479dc4c9",
   "metadata": {},
   "source": [
    "# steps\n",
    "0. initialzz model\n",
    "1. make function to get session history chat\n",
    "2. design prompt and make chain\n",
    "3. make RunnableWithMessageHistory\n",
    "4. make config\n",
    "5. invoke runnablewithmesagehistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84adda",
   "metadata": {},
   "source": [
    "# Managing conversation history\n",
    "Limiting it or else it will keep on growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"I'm good. Tell me about AI.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='AI stands for Artificial Intelligence.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can AI learn by itself?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes, through techniques like machine learning.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i like GENAI part of AI', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# trim_messages = trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count.\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 60,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True, # including systemmessages\n",
    "    allow_partial = False,\n",
    "    start_on = \"human\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hi, I'm Ashish.\"),\n",
    "    AIMessage(content=\"Hello Ashish! How are you?\"),\n",
    "    HumanMessage(content=\"I'm good. Tell me about AI.\"),\n",
    "    AIMessage(content=\"AI stands for Artificial Intelligence.\"),\n",
    "    HumanMessage(content=\"Can AI learn by itself?\"),\n",
    "    AIMessage(content=\"Yes, through techniques like machine learning.\"),\n",
    "    HumanMessage(content=\"i like GENAI part of AI\")\n",
    "]# its kinda chat history\n",
    "\n",
    "\n",
    "\n",
    "trimmer.invoke(messages)# for demo line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c43bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It sounds like you're interested in Generative AI, which focuses on creating new content, such as images, text, music, or even entire virtual environments. This area of AI includes models like OpenAI's GPT, DALL-E, and other similar technologies that generate creative outputs based on the data they've been trained on. Generative AI can be used in various applications, from aiding in art and design to enhancing productivity in generating written content.\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\",\"you ar an expert AI assistant, and answer the user to best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")# and we know MssgPlcholde gets histroy\n",
    "\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)# actual used trimmer here\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"what AI do i like \")],\n",
    "    \"language\":\"English\"\n",
    "})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be24ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap it in messagehistory\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "289676fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It sounds like you're interested in Generative AI, often referred to as GenAI. Generative AI involves models that can create new content, such as text, images, music, or even entire videos, based on the data they have been trained on. Examples include models like GPT, which generates human-like text, and models that create realistic images. These technologies can be used for a variety of applications, including content creation, design, and entertainment.\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":messages+ [HumanMessage(content=\"whatAi do i like?\")],\n",
    "        \"language\":\"english\",\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590603ff",
   "metadata": {},
   "source": [
    "# üîë Core Runnables\n",
    "\n",
    "RunnableSequence ‚Äì Chains multiple runnables in sequence (.pipe() is the shorthand).\n",
    "\n",
    "RunnableMap ‚Äì Runs multiple runnables in parallel, returning a dictionary of outputs.\n",
    "\n",
    "RunnableParallel ‚Äì Similar to RunnableMap, but used for broader fan-out tasks.\n",
    "\n",
    "RunnableLambda ‚Äì Wraps any Python function into a runnable.\n",
    "\n",
    "RunnablePassthrough ‚Äì Passes input through unchanged (often used for debugging or branching).\n",
    "\n",
    "RunnableBranch ‚Äì Conditional execution (like if/else inside a chain).\n",
    "\n",
    "____________________________________________________________________________________________________\n",
    "\n",
    "# üî§ Prompt & Model Runnables\n",
    "\n",
    "ChatPromptTemplate / PromptTemplate ‚Äì Creates structured prompts.\n",
    "\n",
    "RunnableBinding ‚Äì Binds configuration (like stop tokens, temperature) to a runnable.\n",
    "\n",
    "LLM / ChatModel ‚Äì LLMs themselves are runnables (e.g., ChatOpenAI).\n",
    "\n",
    "RunnableWithRetry ‚Äì Adds retry logic to any runnable.\n",
    "\n",
    "____________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "# üì¶ Output & Parsing Runnables\n",
    "\n",
    "StrOutputParser ‚Äì Converts LLM output into a string.\n",
    "\n",
    "JsonOutputParser ‚Äì Parses structured JSON from LLMs.\n",
    "\n",
    "PydanticOutputParser ‚Äì Validates/parses output into Pydantic models.\n",
    "\n",
    "RunnablePick ‚Äì Picks a field from a dictionary (like output[\"answer\"]).\n",
    "\n",
    "____________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "# üìö Retrieval / Tooling\n",
    "\n",
    "RunnableRetriever ‚Äì Wraps a retriever (e.g., from a vectorstore).\n",
    "\n",
    "RunnableParallel with retriever + LLM ‚Äì Often used in RAG pipelines.\n",
    "\n",
    "RunnableTool ‚Äì Wraps tools (APIs, functions) so they can be invoked by an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bd92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
