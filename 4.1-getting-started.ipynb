{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91c0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f8f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")# thsi is fr langsmith trackig\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\" # required for langchain \n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\") # required for langsmith tracking and we have created tis name in dotenv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e2fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002B4FD5C0C70> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002B4FF4CAEC0> root_client=<openai.OpenAI object at 0x000002B4FD5C2B60> root_async_client=<openai.AsyncOpenAI object at 0x000002B4FF4CAE30> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98957b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Generative AI refers to a category of artificial intelligence algorithms that are designed to create new content, such as images, text, music, or other data. These models learn patterns from existing data and then generate new data with similar characteristics. Some common forms of generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** This approach involves two neural networks, a generator and a discriminator, that compete against each other. The generator creates new data instances, while the discriminator evaluates them for authenticity. Over time, the generator improves its ability to create realistic data.\\n\\n2. **Variational Autoencoders (VAEs):** VAEs are a type of deep learning architecture that learns to encode data into a smaller latent space and then decode it back into a new data instance. They are used to generate new, similar data by sampling from the latent space.\\n\\n3. **Transformers:** Models like OpenAI's GPT (Generative Pre-trained Transformer) and Google's BERT have popularized the use of transformer architectures for generating text. They are particularly effective for natural language processing tasks, such as text generation, translation, and summarization.\\n\\n4. **Diffusion Models:** Emerging in fields like image and video generation, diffusion models generate data by iteratively transforming random noise into coherent output.\\n\\nGenerative AI has a wide range of applications, including creative industries like art and music, video game development, virtual world creation, and even drug discovery and design in the scientific community. It can assist in prototyping concepts quickly, adding variability to simulated environments, and personalizing user experiences. However, attention to ethical considerations and potential misuse, such as generating deepfakes or misleading information, is crucial in the development and deployment of generative AI technologies.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 353, 'prompt_tokens': 13, 'total_tokens': 366, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bz3LSSTJCxojT1zwb4wVkNY5qpXLB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fffa9853-0593-4356-b5f1-264fd7452067-0', usage_metadata={'input_tokens': 13, 'output_tokens': 353, 'total_tokens': 366, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input and get response\n",
    "result = llm.invoke(\"what is generative AI?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3524d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert AI engineer. Provide a concise answer to the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# from_messages is used to create multiple messages in the chat prompt \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are an expert AI engineer. Provide a concise answer to the question.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]   \n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254558f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmith is a platform developed by LangChain that assists developers in building, testing, debugging, and evaluating applications driven by large language models (LLMs). It provides tools for tracking and managing prompt performance, debugging complex workflows, and understanding model outputs to enhance application reliability and effectiveness. Essentially, LangSmith streamlines the development process for applications utilizing LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 34, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-Bz3Lby3gB3ByIWflTAn4IduPJZ7TA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--60219fca-a2f3-46b8-ae99-00f1b390202b-0' usage_metadata={'input_tokens': 34, 'output_tokens': 72, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# create a chain\n",
    "# chain means combine anything in langchain, like prompt, llm, vectorstore, retriever, etc.\n",
    "chain = prompt | llm # | is the pipe operator in langchain to connect components\n",
    "response = chain.invoke({\"input\": \"can you tell me about langsmith?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40bf4a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8881e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool provided by LangChain, designed to enhance the development, testing, and monitoring of language models and applications built using the LangChain framework. It offers a comprehensive suite for debugging, evaluating, and deploying applications, streamlining the iterative process of building with AI. LangSmith facilitates improved efficiency by enabling developers to better understand model performance and optimize their language-based projects.\n"
     ]
    }
   ],
   "source": [
    "# output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"can you tell me about langsmith?\"})\n",
    "print(response)\n",
    "# only get the string output, not the full response object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c35c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc3305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
