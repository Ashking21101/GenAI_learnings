{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78884345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")# thsi is fr langsmith trackig\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\" # required for langchain \n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\") # required for langsmith tracking and we have created tis name in dotenv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009c0563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000215BE5B3760> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000215C24A6CB0> root_client=<openai.OpenAI object at 0x00000215BE5B2920> root_async_client=<openai.AsyncOpenAI object at 0x00000215C24A6C20> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926b4cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x215be5b3f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data ingestion from website https://docs.smith.langchain.com/administration/tutorials/manage_spend\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "\n",
    "loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5535400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nOptimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationTutorialsOptimize tracing spend on LangSmithHow-to GuidesSetupConceptual GuideSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceAdministrationTutorialsOptimize tracing spend on LangSmithOn this pageOptimize tracing spend on LangSmith\\nRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs\\n\\nnoteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):\\n\\nUnderstand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).\\n\\nThe first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nInvoices\\u200b\\nWe understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month\\'s\\ninvoice, which shows your running spend thus far this month.\\n\\nnoteLangSmith\\'s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above GIF, we see that the charges for LangSmith Traces are broken up by \"tenant_id\" (i.e. Workspace ID), meaning we can track tracing spend\\non each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production\\nworkspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, meaning that, by default, your traces are retained for 400 days\\nYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs)\\n\\nGiven that the number of total traces per day is equal to the number of extended retention traces per day, it\\'s most likely the\\ncase that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings.\\nOptimization 1: manage data retention\\u200b\\nLangSmith charges differently based on a trace\\'s data retention (see our data retention conceptual docs),\\nwhere short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will\\nshow how to get optimal settings for data retention without sacrificing historical observability, and\\nshow the effect it has on our bill.\\nChange org level retention defaults for new projects\\u200b\\nNavigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in our org.\\nnoteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024\\nhave this defaulted to Base.\\n\\nChange project level retention defaults\\u200b\\nData retention settings are adjustable per project on the tracing project page.\\nNavigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.\\n\\nApply extended data retention to a percentage of traces\\u200b\\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:\\n\\n10% of all traces: For general analysis or analyzing trends long-term\\nErrored traces: To thoroughly investigate and debug issues\\nTraces with specific metadata: For long-term examination of particular features or user flows\\n\\nTo configure this:\\n\\nNavigate to Projects > Your project name > Select + New > Select New Automation\\nName your rule and optionally apply filters or a sample rate. For more information on configuring filters, see filtering techniques\\n\\nnoteWhen an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.\\nFor example, this is what the configuration looks like to keep 10% of all traces for extended data retention:\\n\\nIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run\\nrule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),\\nand will persist indefinitely, even after the trace gets deleted.\\nSee results after 7 days\\u200b\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, we can see thatwe\\'ve only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat\\'s a cost reduction of nearly 75% per day!\\n\\nOptimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:\\n\\nCurrent Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:\\n\\nnoteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.\\n\\nAs we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.\\nnoteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\\nuse this feature, please read more about its functionality here.\\nSet dev/staging limits and view total spent limit across workspaces\\u200b\\nFollowing the same logic for our dev and staging environments, we set limits at 10% of the production\\nlimit on usage for each workspace.\\nWhile this works with our usage pattern, setting good dev and staging limits may vary depending on\\nyour use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:\\n\\nWith this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.\\nSummary\\u200b\\nIn this tutorial, we learned how to:\\n\\nCut down our existing costs with data retention policies\\nPrevent future overspend with usage limits\\n\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTutorialsNextAdministration how-to guidesProblem SetupUnderstand your current usageUsage GraphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsApply extended data retention to a percentage of tracesSee results after 7 daysOptimization 2: limit usageSetting a good total traces limitCutting maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummaryCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e07581",
   "metadata": {},
   "source": [
    "load data---->docs--->divide in chunks---->text---->vector----->vector embedding-----vector store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bc31a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationTutorialsOptimize tracing spend on LangSmithHow-to GuidesSetupConceptual GuideSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceAdministrationTutorialsOptimize tracing spend on LangSmithOn this pageOptimize tracing spend on LangSmith\\nRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nInvoices\\u200b\\nWe understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month's\\ninvoice, which shows your running spend thus far this month.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteLangSmith\\'s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above GIF, we see that the charges for LangSmith Traces are broken up by \"tenant_id\" (i.e. Workspace ID), meaning we can track tracing spend\\non each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production\\nworkspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, meaning that, by default, your traces are retained for 400 days\\nYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the\\ncase that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings.\\nOptimization 1: manage data retention\\u200b\\nLangSmith charges differently based on a trace's data retention (see our data retention conceptual docs),\\nwhere short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will\\nshow how to get optimal settings for data retention without sacrificing historical observability, and\\nshow the effect it has on our bill.\\nChange org level retention defaults for new projects\\u200b\\nNavigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in our org.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024\\nhave this defaulted to Base.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Change project level retention defaults\\u200b\\nData retention settings are adjustable per project on the tracing project page.\\nNavigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.\\n\\nApply extended data retention to a percentage of traces\\u200b\\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:\\n\\n10% of all traces: For general analysis or analyzing trends long-term\\nErrored traces: To thoroughly investigate and debug issues\\nTraces with specific metadata: For long-term examination of particular features or user flows\\n\\nTo configure this:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='To configure this:\\n\\nNavigate to Projects > Your project name > Select + New > Select New Automation\\nName your rule and optionally apply filters or a sample rate. For more information on configuring filters, see filtering techniques\\n\\nnoteWhen an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.\\nFor example, this is what the configuration looks like to keep 10% of all traces for extended data retention:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"If you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run\\nrule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),\\nand will persist indefinitely, even after the trace gets deleted.\\nSee results after 7 days\\u200b\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, we can see thatwe've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat's a cost reduction of nearly 75% per day!\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Current Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.\\nnoteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\\nuse this feature, please read more about its functionality here.\\nSet dev/staging limits and view total spent limit across workspaces\\u200b\\nFollowing the same logic for our dev and staging environments, we set limits at 10% of the production\\nlimit on usage for each workspace.\\nWhile this works with our usage pattern, setting good dev and staging limits may vary depending on\\nyour use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.\\nSummary\\u200b\\nIn this tutorial, we learned how to:\\n\\nCut down our existing costs with data retention policies\\nPrevent future overspend with usage limits'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Cut down our existing costs with data retention policies\\nPrevent future overspend with usage limits\\n\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTutorialsNextAdministration how-to guidesProblem SetupUnderstand your current usageUsage GraphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsApply extended data retention to a percentage of tracesSee results after 7 daysOptimization 2: limit usageSetting a good total traces limitCutting maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummaryCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to divide the documents into smaller chunks, so that we can use them in retriever because retriever works on smaller chunks of text\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891fa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings() # text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab7ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(docs, embeddings) # vector store db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90662d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x215d1a4b7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f967fe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"LangSmith has two usage limits: total traces and extended\"\n",
    "result = vector_store.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d9ae7",
   "metadata": {},
   "source": [
    "User Input ‚Üí Prompt Template ‚Üí LLM ‚Üí Output\n",
    "\n",
    "\n",
    "Key Components of a Chain\n",
    "Prompt Template\n",
    "Structures the input for the model (e.g., using ChatPromptTemplate).\n",
    "\n",
    "LLM or ChatModel\n",
    "The actual language model used (e.g., OpenAI‚Äôs GPT, Anthropic‚Äôs Claude, etc.).\n",
    "\n",
    "Output Parser (optional)\n",
    "Parses the LLM's output into structured data (e.g., JSON, Pydantic models).\n",
    "\n",
    "Memory (optional)\n",
    "Stores chat history or contextual information across interactions.\n",
    "\n",
    "Tools (optional)\n",
    "Enables agents or chains to interact with external tools or APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f8c31",
   "metadata": {},
   "source": [
    "# retrieval chain (below Chain style is for RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c104aad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the context below.\\n    <context>\\n    {context}\\n    </context>    \\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000215BE5B3760>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000215C24A6CB0>, root_client=<openai.OpenAI object at 0x00000215BE5B2920>, root_async_client=<openai.AsyncOpenAI object at 0x00000215C24A6C20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using retrieval chain, document chaining, and llm\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# from_template is used to create one line prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based on the context below.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "doc_chain\n",
    "\n",
    "## thi doc_chain will take the query and context and return the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d02480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What happens if a user exceeds the extended traces limit for a specific project in LangSmith?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this piece is for demo purpose , not used further\n",
    "# here you can see that we are passing the context manually, but in real world application, we will use retriever to get the context dynamically based on the query \n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "doc_chain.invoke({\n",
    "    \"input\":\"what is LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\": [Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. The total traces limit is the maximum number of traces that can be stored in LangSmith, while the extended traces limit is the maximum number of traces that can be stored in LangSmith for a specific project. The total traces limit is 100,000 traces, while the extended traces limit is 10,000 traces per project.\")]\n",
    "})\n",
    "# context added manually \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a15b3b",
   "metadata": {},
   "source": [
    "however we want the docuents to first come from the retriever we just set up. That way , we can use the retriever to dynamically select the most relevant documents and pass those in for a given question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b7e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input--->retriever--->vectorDB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f23ce",
   "metadata": {},
   "source": [
    "Essentially, retrieval_chain orchestrates the entire process:\n",
    "\n",
    "input --> retriever (gets context) --> doc_chain (uses context and original input to generate answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1137f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()# so we are making the vector store as context \n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8efc40cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000215D1A4B7F0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the context below.\\n    <context>\\n    {context}\\n    </context>    \\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000215BE5B3760>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000215C24A6CB0>, root_client=<openai.OpenAI object at 0x00000215BE5B2920>, root_async_client=<openai.AsyncOpenAI object at 0x00000215C24A6C20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain # it is just a chain that takes the input and returns the answer based on the context from the retriever\n",
    "\n",
    "# main part is retriever it retreves relevant content from vector store based on the query and then passes it to the doc_chain which returns the answer based on the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e15060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, if you have any specific questions on setting usage limits, understanding your current usage with LangSmith, or managing spend using LangSmith's usage metrics, feel free to ask! The context outlines the process of setting up usage limits, understanding spend via usage graphs and invoices, and the importance of managing these metrics for environments like production that incur the majority of the costs.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the answer from the retriever\n",
    "response = retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db6eb276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(id='ac4606b6-fbc4-413f-b99c-d40cda6176e3', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'),\n",
       "  Document(id='b464d6b1-fd88-4fff-ad63-b31bb67c9ec6', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nInvoices\\u200b\\nWe understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month's\\ninvoice, which shows your running spend thus far this month.\"),\n",
       "  Document(id='394af5ec-bbe4-45d1-a8da-806d64d36b1e', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       "  Document(id='0e44768c-50a9-45a1-9c79-75529833d9e9', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:')],\n",
       " 'answer': \"Based on the context provided, if you have any specific questions on setting usage limits, understanding your current usage with LangSmith, or managing spend using LangSmith's usage metrics, feel free to ask! The context outlines the process of setting up usage limits, understanding spend via usage graphs and invoices, and the importance of managing these metrics for environments like production that incur the majority of the costs.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883c424",
   "metadata": {},
   "source": [
    "# Understanding\n",
    "\n",
    "vector_store = docs is embedded\n",
    "\n",
    "prompt = from_template(\"\"\"...{context}\"\"\")\n",
    "\n",
    "docs_chain = create_stuff_document_chain(prompt + llm)\n",
    "\n",
    "retriever = vectorDB #this search things from vectorstore that are relevant t our qestion and pass it as our context in prompt\n",
    "\n",
    "retrieval_chain = retriever + docs_chain (just a chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deedef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6d0efb",
   "metadata": {},
   "source": [
    "This is where the magic happens. It creates a complete workflow:\n",
    "\n",
    "Step 1 (Retrieval): When retrieval_chain.invoke({\"input\": \"your question\"}) is called, the retrieval_chain first uses the retriever to search for documents based on \"your question\". The retriever will find the document that talks about \"LangSmith usage limits\" because it matches your input.\n",
    "\n",
    "Step 2 (Context Passing): The documents found by the retriever are then automatically passed as the context argument to the doc_chain. You don't see this explicit passing in your retrieval_chain.invoke() call because create_retrieval_chain handles it internally.\n",
    "\n",
    "Step 3 (Document Stuffing/Answering): The doc_chain then takes these retrieved documents (as its context) and your original \"input\" (your question) and uses the llm to formulate an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc885be7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f3bb0c",
   "metadata": {},
   "source": [
    "This \"input\" (your question) is used in two main ways within the retrieval_chain:\n",
    "\n",
    "By the retriever: The retriever uses this question to search your vector_store and find the most relevant documents (your context).\n",
    "\n",
    "By the doc_chain: Once the relevant documents are found, the doc_chain receives both these documents (as context) and your original question (as input) to formulate the final answer using the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ef8e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
